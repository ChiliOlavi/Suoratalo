import{e as T,r as g,p as w,R,j as f,a as S}from"./pipelines-DTrqgdQT.js";T.allowRemoteModels=!0;const M="Xenova/bert-base-multilingual-cased",C=50;function I(){console.log("App mounted");const c=g.useRef(null);g.useEffect(()=>{async function e(){c.current=await w("fill-mask",M),console.log("Model loaded, starting mixing");const t=Array.from(document.querySelectorAll(".container p"));console.log("Found paragraphs:",t.length);for(const a of t)console.log("Processing paragraph:",a.textContent.slice(0,60),"..."),k(a)}e()},[]);async function k(e){if(e.dataset.mixing==="true")return;e.dataset.mixing="true";const t=c.current,a=e.textContent;let l=[...(await t.tokenizer(a,{addSpecialTokens:!0})).input_ids.data],o=[];for(let n=1;n<l.length-1;++n)o.push(n);let s=[...l],d=0;for(e.textContent=i(a);o.length>0&&d<20;){const n=o[Math.floor(Math.random()*o.length)],p=[...s];p[n]=t.tokenizer.mask_token_id;const x=await t.tokenizer.decode(p,{skipSpecialTokens:!1}),u=await t(x,{top_k:C});u.length>0&&(s[n]=u[0].token),o=o.filter(r=>r!==n),d+=1;const h=await t.tokenizer.decode(s,{skipSpecialTokens:!1});e.textContent=i(h),await new Promise(r=>setTimeout(r,100))}const m=await t.tokenizer.decode(s,{skipSpecialTokens:!1});e.textContent=i(m),e.dataset.mixing="false"}function i(e){return e.replace(/\[CLS\]/g,"").replace(/\[SEP\]/g,"").replace(/\[UNK\]/g,"").replace(/\s+/g," ").trim()}return null}R.createRoot(document.getElementById("root")).render(f.jsx(S.StrictMode,{children:f.jsx(I,{})}));
