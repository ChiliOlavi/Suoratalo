import{e as R,r as o,j as M,p as z}from"./pipelines-DTrqgdQT.js";R.allowRemoteModels=!0;const b="Xenova/bert-base-multilingual-cased",j=50;function S(t){return t.replace(/\[CLS\]/g,"").replace(/\[SEP\]/g,"").replace(/\[UNK\]/g,"").replace(/\s+/g," ").trim()}function D({initialText:t,modelName:p=b,topKPredictions:f=j}){const[w,i]=o.useState(t),[c,k]=o.useState(!0),[x,g]=o.useState(!1),d=o.useRef(null);o.useEffect(()=>{(async()=>{try{k(!0),d.current=await z("fill-mask",p)}catch(a){console.error("Failed to load pipeline:",a),i("Error loading model. Please check console.")}finally{k(!1)}})()},[p]);const m=o.useCallback(async()=>{if(c||x||!d.current)return;g(!0);const s=d.current;try{const a=t,_=await s.tokenizer(a,{addSpecialTokens:!0});console.log("Text is mixing.");let n=[..._.input_ids.data],r=[];for(let e=1;e<n.length-1;++e)r.push(e);let T=0;for(;r.length>0&&T<20;){const e=r[Math.floor(Math.random()*r.length)],h=[...n];h[e]=s.tokenizer.mask_token_id;const P=await s.tokenizer.decode(h,{skipSpecialTokens:!1}),u=await s(P,{top_k:f});if(u.length>0){const l=n[e],E=u.find(L=>L.token!==l);E?n[e]=E.token:n[e]=u[0].token}r=r.filter(l=>l!==e),T+=1;const y=await s.tokenizer.decode(n,{skipSpecialTokens:!1});i(S(y)),await new Promise(l=>setTimeout(l,100))}const I=await s.tokenizer.decode(n,{skipSpecialTokens:!1});i(S(I))}catch(a){console.error("Error during text mixing:",a),i("Error mixing text. Please check console.")}finally{g(!1)}},[t,c,x,p,f]);return o.useEffect(()=>{!c&&d.current&&(i(t),m())},[t,c,m]),M.jsx("div",{children:M.jsx("p",{children:w})})}export{D as T};
