import{e as R,r as s,j as M,p as z}from"./pipelines-DTrqgdQT.js";R.allowRemoteModels=!0;const b="Xenova/bert-base-multilingual-cased",j=50;function S(t){return t.replace(/\[CLS\]/g,"").replace(/\[SEP\]/g,"").replace(/\[UNK\]/g,"").replace(/\s+/g," ").trim()}function D({initialText:t,modelName:p=b,topKPredictions:f=j}){const[w,a]=s.useState(t),[c,k]=s.useState(!0),[g,x]=s.useState(!1),d=s.useRef(null);s.useEffect(()=>{(async()=>{try{k(!0),d.current=await z("fill-mask",p)}catch(i){console.error("Failed to load pipeline:",i),a("Error loading model. Please check console.")}finally{k(!1)}})()},[p]);const m=s.useCallback(async()=>{if(c||g||!d.current)return;x(!0);const n=d.current;try{const i=t,_=await n.tokenizer(i,{addSpecialTokens:!0});console.log("Text is mixing.");let o=[..._.input_ids.data],r=[];for(let e=1;e<o.length-1;++e)r.push(e);let T=0;for(;r.length>0&&T<20;){const e=r[Math.floor(Math.random()*r.length)],h=[...o];h[e]=n.tokenizer.mask_token_id;const P=await n.tokenizer.decode(h,{skipSpecialTokens:!1}),u=await n(P,{top_k:f});if(u.length>0){const l=o[e],E=u.find(L=>L.token!==l);E?o[e]=E.token:o[e]=u[0].token}r=r.filter(l=>l!==e),T+=1;const y=await n.tokenizer.decode(o,{skipSpecialTokens:!1});a(S(y)),await new Promise(l=>setTimeout(l,100))}const I=await n.tokenizer.decode(o,{skipSpecialTokens:!1});a(S(I))}catch(i){console.error("Error during text mixing:",i),a("Error mixing text. Please check console.")}finally{x(!1)}},[t,c,g,p,f]);return s.useEffect(()=>{a(t)},[t]),s.useEffect(()=>{!c&&d.current&&m()},[t,c,m]),M.jsx("div",{children:M.jsx("p",{children:w})})}export{D as T};
