import{e as L,r as n,j as x,p as P,c as R}from"./pipelines-DTrqgdQT.js";L.allowRemoteModels=!0;const j="Xenova/bert-base-multilingual-cased",z=50;function f(e){return e.replace(/\[CLS\]/g,"").replace(/\[SEP\]/g,"").replace(/\[UNK\]/g,"").replace(/\s+/g," ").trim()}function b({initialText:e,modelName:s=j,topKPredictions:u=z}){const[S,r]=n.useState(e),[c,k]=n.useState(!0),[g,m]=n.useState(!1),d=n.useRef(null);n.useEffect(()=>{(async()=>{try{k(!0),d.current=await P("fill-mask",s),console.log(`Model "${s}" loaded.`)}catch(i){console.error("Failed to load pipeline:",i),r("Error loading model. Please check console.")}finally{k(!1)}})()},[s]);const T=n.useCallback(async()=>{if(c||g||!d.current)return;m(!0);const t=d.current;try{const i=e,w=await t.tokenizer(i,{addSpecialTokens:!0});console.log("Text is mixing.");let l=[...w.input_ids.data],a=[];for(let o=1;o<l.length-1;++o)a.push(o);let h=0;for(r(f(i));a.length>0&&h<20;){const o=a[Math.floor(Math.random()*a.length)],E=[...l];E[o]=t.tokenizer.mask_token_id;const y=await t.tokenizer.decode(E,{skipSpecialTokens:!1}),M=await t(y,{top_k:u});M.length>0&&(l[o]=M[0].token),a=a.filter(p=>p!==o),h+=1;const I=await t.tokenizer.decode(l,{skipSpecialTokens:!1});r(f(I)),await new Promise(p=>setTimeout(p,100))}const _=await t.tokenizer.decode(l,{skipSpecialTokens:!1});r(f(_))}catch(i){console.error("Error during text mixing:",i),r("Error mixing text. Please check console.")}finally{m(!1)}},[e,c,g,s,u]);return n.useEffect(()=>{!c&&d.current&&(r(e),T())},[e,c,T]),x.jsx("div",{children:x.jsx("p",{children:S})})}document.querySelectorAll("p.mix-me").forEach(e=>{const s=e.textContent;e.innerHTML="",R.createRoot(e).render(x.jsx(b,{initialText:s}))});
