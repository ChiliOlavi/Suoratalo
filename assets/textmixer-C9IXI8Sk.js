import{e as y,r as o,j as M,p as L}from"./pipelines-DTrqgdQT.js";y.allowRemoteModels=!0;const R="Xenova/bert-base-multilingual-cased",z=50;function u(e){return e.replace(/\[CLS\]/g,"").replace(/\[SEP\]/g,"").replace(/\[UNK\]/g,"").replace(/\s+/g," ").trim()}function j({initialText:e,modelName:d=R,topKPredictions:f=z}){const[S,n]=o.useState(e),[l,k]=o.useState(!0),[x,g]=o.useState(!1),c=o.useRef(null);o.useEffect(()=>{(async()=>{try{k(!0),c.current=await L("fill-mask",d)}catch(r){console.error("Failed to load pipeline:",r),n("Error loading model. Please check console.")}finally{k(!1)}})()},[d]);const m=o.useCallback(async()=>{if(l||x||!c.current)return;g(!0);const t=c.current;try{const r=e,w=await t.tokenizer(r,{addSpecialTokens:!0});console.log("Text is mixing.");let i=[...w.input_ids.data],a=[];for(let s=1;s<i.length-1;++s)a.push(s);let T=0;for(n(u(r));a.length>0&&T<20;){const s=a[Math.floor(Math.random()*a.length)],h=[...i];h[s]=t.tokenizer.mask_token_id;const I=await t.tokenizer.decode(h,{skipSpecialTokens:!1}),E=await t(I,{top_k:f});E.length>0&&(i[s]=E[0].token),a=a.filter(p=>p!==s),T+=1;const P=await t.tokenizer.decode(i,{skipSpecialTokens:!1});n(u(P)),await new Promise(p=>setTimeout(p,100))}const _=await t.tokenizer.decode(i,{skipSpecialTokens:!1});n(u(_))}catch(r){console.error("Error during text mixing:",r),n("Error mixing text. Please check console.")}finally{g(!1)}},[e,l,x,d,f]);return o.useEffect(()=>{!l&&c.current&&(n(e),m())},[e,l,m]),M.jsx("div",{children:M.jsx("p",{children:S})})}export{j as T};
